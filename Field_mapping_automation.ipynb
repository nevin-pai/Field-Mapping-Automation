{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_mapping(df_1, df_2, col_1, col_2):\n",
    "    column_mapping_value = len(df_1[df_1[col_1].isin(df_2[col_2])])\n",
    "    df_1_mapped_unique_record_indentifier = df_1[df_1[col_1].isin(df_2[col_2])]['unique_record_identifier'].tolist()\n",
    "    df_2_mapped_unique_record_indentifier = df_2[df_2[col_2].isin(df_1[col_1])]['unique_record_identifier'].tolist()\n",
    "#     column_mapping_value = df_1[df_1[col_1].isin(df_2[col_2])][col_1].nunique()\n",
    "    return column_mapping_value, df_1_mapped_unique_record_indentifier, df_2_mapped_unique_record_indentifier\n",
    "\n",
    "def mapping_matrix(df_1, df_2):\n",
    "    mapping_arr=[]\n",
    "    for col_1 in df_1.columns:\n",
    "        single_column_mapping_arr = []\n",
    "        single_column_mapping_arr.append(col_1)\n",
    "        for col_2 in df_2.columns:\n",
    "            column_mapping_value = column_mapping(df_1, df_2, col_1, col_2)\n",
    "            single_column_mapping_arr.append(column_mapping_value)\n",
    "        mapping_arr.append(single_column_mapping_arr)\n",
    "        \n",
    "    # Create a dataset\n",
    "    columns = df_2.columns.tolist()\n",
    "    columns.insert(0, 'df_1')\n",
    "    matrix_df = pd.DataFrame(mapping_arr, columns=columns)\n",
    "    matrix_df.set_index(\"df_1\", inplace=True)\n",
    "    matrix_df.to_csv('mapping_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb019da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_time(one_field_runtime, no_of_columns):\n",
    "    current_time = datetime.datetime.now()\n",
    "    hr = current_time.hour\n",
    "    minute = current_time.minute\n",
    "    sec = current_time.second\n",
    "    print(\"one_field_runtime\", one_field_runtime)\n",
    "    print(\"no_of_columns\", no_of_columns)\n",
    "    total_runtime = (one_field_runtime * no_of_columns)/60\n",
    "    print(\"total_runtime\", total_runtime)\n",
    "    minute = minute + total_runtime\n",
    "    if minute >= 60:\n",
    "        hr+=1\n",
    "        minute = minute - 60\n",
    "        if minute == 60:\n",
    "            minute=='00'\n",
    "    if len(str(minute))==1:\n",
    "        minute = '0'+str(minute)\n",
    "    print(\"\\t\\t\\t\\tEstimated Time: \" + str(hr) + \":\" + str(minute))\n",
    "    \n",
    "\n",
    "def random_mapping(df_1, df_2, coverage=0, final_arr=[]):\n",
    "    no_of_columns = len(df_1.columns.tolist())\n",
    "    cnt=0\n",
    "    df_1_mapped_unique_indentifier_arr = []\n",
    "    df_2_mapped_unique_indentifier_arr = []\n",
    "    for x in df_1.columns:\n",
    "        if x != 'unique_record_identifier':\n",
    "            if cnt == 0:\n",
    "                t3 = time.time()\n",
    "            if cnt == 1:\n",
    "                t4 = time.time()\n",
    "                one_field_runtime = round((t4-t3))\n",
    "    #             estimated_time(one_field_runtime, no_of_columns)\n",
    "            cnt+=1\n",
    "            if(cnt%10==0):\n",
    "                print(\"Checkpoint: \" + str(cnt) + \"/\" + str(no_of_columns))\n",
    "            mapping_value = 0\n",
    "            mapping_df_1_column = mapping_df_2_column= ''\n",
    "            for col_1 in df_1.columns:\n",
    "                if col_1 != 'unique_record_identifier':\n",
    "                    for col_2 in df_2.columns:\n",
    "                        if col_2 != 'unique_record_identifier':\n",
    "                            column_mapping_value, df_1_mapped_indentifier, df_2_mapped_indentifier = column_mapping(df_1, df_2, col_1, col_2)\n",
    "                            if column_mapping_value > mapping_value:\n",
    "                                mapping_value = column_mapping_value\n",
    "                                df_1_mapped_unique_indentifier = df_1_mapped_indentifier\n",
    "                                df_2_mapped_unique_indentifier = df_2_mapped_indentifier\n",
    "                                mapping_df_1_column = col_1\n",
    "                                mapping_df_2_column = col_2\n",
    "            if (mapping_df_1_column != ''):\n",
    "                final_mapping = mapping_df_1_column + \" ---> \" + mapping_df_2_column + \" = \" + str(mapping_value)\n",
    "                df_1_mapped_unique_indentifier_arr.append(df_1_mapped_unique_indentifier)\n",
    "                df_2_mapped_unique_indentifier_arr.append(df_2_mapped_unique_indentifier)\n",
    "#                 print(final_mapping)\n",
    "                final_arr.append(final_mapping)\n",
    "                temp_1 = pd.DataFrame(df_1[mapping_df_1_column].unique().tolist(), columns={mapping_df_1_column}, dtype='O')\n",
    "                temp_2 = pd.DataFrame(df_2[mapping_df_2_column].unique().tolist(), columns={mapping_df_2_column}, dtype='O')\n",
    "                temp_df=pd.merge(temp_1, temp_2, left_on=mapping_df_1_column, right_on=mapping_df_2_column)\n",
    "                df_1_length_before = len(df_1)\n",
    "                df_1 = df_1[~df_1[mapping_df_1_column].isin(temp_df[mapping_df_1_column])]\n",
    "                df_2 = df_2[~df_2[mapping_df_2_column].isin(temp_df[mapping_df_1_column])]\n",
    "                df_1_length_after = len(df_1)\n",
    "                coverage += df_1_length_before - df_1_length_after\n",
    "    return coverage, df_1, df_2, final_arr, df_1_mapped_unique_indentifier_arr, df_2_mapped_unique_indentifier_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_remove(df, remove_field_arr):\n",
    "    df.drop(remove_field_arr, errors='ignore', inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf06313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(df, i=0):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "    df = df.dropna(how='all')\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].map(str)\n",
    "        df[col] = df[col].apply(lambda x: x.lower())\n",
    "#         df[col] = df[col].replace('nan', str(i)+\"_nan\")\n",
    "#         i+=1\n",
    "    audit_field = ['source_type', 'source_name', 'original_string', 'source_uuid', 'event_timestamp_epoch', 'event_timeoffset', 'event_timestamp_iso_client_timezone', 'event_timestamp_day', 'event_timestamp_month', 'event_timestamp_year', 'event_timestamp_week_day', 'event_timestamp_week_index', 'event_timestamp_hour', 'event_timestamp_minute', 'event_timestamp_date', 'parsed_timestamp', 'ingested_timestamp', 'analysis_period_start', 'analysis_period_end']\n",
    "    for col in df.columns:\n",
    "        unique_col_values = str(df[col].unique().tolist())\n",
    "        if ((unique_col_values == \"['false', 'true']\") | (unique_col_values == \"['true', 'false']\") | (unique_col_values == \"['nan', 'true', 'false']\") |(unique_col_values == \"['nan', 'false', 'true']\") | (unique_col_values== \"['false']\") | (unique_col_values== \"['nan', 'false']\") | (unique_col_values== \"['true']\") | (unique_col_values== \"['nan', 'true']\") | (unique_col_values== \"['true', 'nan']\") | (unique_col_values== \"['false', 'nan']\") | (unique_col_values == \"['true', 'false', 'nan']\") | (unique_col_values == \"['false', 'true', 'nan']\")):\n",
    "            audit_field.append(col)\n",
    "        if ((('unnamed' in col) | ('epoch' in col) | ('time' in col) | ('date' in col)) & (col not in audit_field)):\n",
    "            audit_field.append(col)  \n",
    "    df = field_remove(df, audit_field)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace('nan', str(i)+\"_nan\")\n",
    "        i+=1\n",
    "#     df.drop(audit_field, errors='ignore', inplace=True, axis=1)\n",
    "    return df, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_to_keep(df, col_to_keep):\n",
    "    col_to_keep = col_to_keep[1:-1].split(\",\")\n",
    "    col_to_keep = [x.strip() for x in col_to_keep]\n",
    "    df = df[col_to_keep]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ade7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_summary(df_1_name, df_1, coverage, total_duplicate_records, time_taken=0):\n",
    "#     print(\"\\n***************************************************************************\\n\")\n",
    "    print(colored(\"\\n\\nMAPPING SUMMARY:\", attrs=['bold']))\n",
    "    print(\"Number of records in (\"+ df_1_name + \"): \" + str(len(df_1)))\n",
    "    print(\"Number of records matched: \", coverage)\n",
    "    print(\"Number of records not matched: \", len(df_1) - coverage)\n",
    "    print(\"Number of duplicate records: \", total_duplicate_records)\n",
    "    print(\"Coverage:\", str(\"{:.2f}\".format(round((coverage/len(df_1))*100)) + \"%\"))\n",
    "    print(\"Accuracy:\", str(\"{:.2f}\".format(((len(df_1) - total_duplicate_records)/len(df_1))*100) + \"%\"))\n",
    "    if time_taken!=0:\n",
    "        print(\"Time taken to run:\", str(time_taken) + \"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(final_arr, df_1_name, df_2_name):\n",
    "    print(colored(\"\\nMAPPING DETAILS (\"+ df_1_name + \" \"+\"--->\"+ \" \"+ df_2_name + \"):\", attrs=['bold']))\n",
    "    for x in final_arr:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7db4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_field_details(mapping_type, final_arr):\n",
    "    mapping_df_1_col_list = []\n",
    "    mapping_df_2_col_list = []\n",
    "    mapping_df_1_duplicate_col_list = []\n",
    "    mapping_df_2_duplicate_col_list = []\n",
    "    \n",
    "    for mapping_col in final_arr:\n",
    "        mapping_df_1_col_list.append(mapping_type +'_secondary_mapped_' +  mapping_col.split(\" \")[0])\n",
    "        mapping_df_2_col_list.append(mapping_type +'_primary_mapped_' +  mapping_col.split(\" \")[2])\n",
    "        mapping_df_1_duplicate_col_list.append(mapping_type +'_secondary_mapped_' +  mapping_col.split(\" \")[0] + '_duplicate')\n",
    "        mapping_df_2_duplicate_col_list.append(mapping_type +'_primary_mapped_' +  mapping_col.split(\" \")[2] + '_duplicate')\n",
    "        \n",
    "    mapped_arr = [mapping_type + '_secondary_mapped', mapping_type + '_primary_mapped']\n",
    "    duplicate_arr = [mapping_type + '_secondary_duplicate', mapping_type + '_primary_duplicate']\n",
    "    total_duplicate_records = []\n",
    "    \n",
    "    globals()[mapped_arr[0]] = pd.DataFrame()\n",
    "    globals()[duplicate_arr[0]] = []\n",
    "    globals()[mapped_arr[1]] = pd.DataFrame()\n",
    "    globals()[duplicate_arr[1]] = []\n",
    "\n",
    "    for i in range(len(final_arr)):\n",
    "        df_1_id = df_1_mapped_unique_indentifier[i]\n",
    "        df_2_id = df_2_mapped_unique_indentifier[i]\n",
    "        mapped_df_1 = df_1_original[df_1_original['unique_record_identifier'].isin(df_1_id)]\n",
    "        mapped_df_2 = df_2_original[df_2_original['unique_record_identifier'].isin(df_2_id)]\n",
    "        globals()[mapped_arr[0]] = pd.concat([globals()[mapped_arr[0]], mapped_df_1])\n",
    "        globals()[mapped_arr[1]] = pd.concat([globals()[mapped_arr[1]], mapped_df_2])\n",
    "        globals()[mapping_df_1_col_list[i]] = mapped_df_1\n",
    "        globals()[mapping_df_2_col_list[i]] = mapped_df_2\n",
    "        \n",
    "        arr=[]\n",
    "        column_name_secondary = final_arr[i].split(\" \")[0]\n",
    "        column_name_primary = final_arr[i].split(\" \")[2]      \n",
    "        globals()[mapping_df_1_duplicate_col_list[i]] = globals()[mapping_df_1_col_list[i]].groupby(column_name_secondary).filter(lambda x: len(x) > 1)[column_name_secondary].unique().tolist()\n",
    "        if len(globals()[mapping_df_1_duplicate_col_list[i]]) > 0:\n",
    "            globals()[duplicate_arr[0]].append(column_name_secondary)\n",
    "            total_duplicate_records.extend(globals()[mapping_df_1_duplicate_col_list[i]])\n",
    "            \n",
    "        globals()[mapping_df_2_duplicate_col_list[i]] = globals()[mapping_df_2_col_list[i]].groupby(column_name_primary).filter(lambda x: len(x) > 1)[column_name_primary].unique().tolist()\n",
    "        if len(globals()[mapping_df_2_duplicate_col_list[i]]) > 0:\n",
    "            globals()[duplicate_arr[1]].append(column_name_primary)\n",
    "            total_duplicate_records.extend(globals()[mapping_df_2_duplicate_col_list[i]])\n",
    "    return total_duplicate_records\n",
    "\n",
    "def duplicate_records_summary(mapping_type):\n",
    "    print(colored(\"\\n\\nDUPLICATE SUMMARY:\", attrs=['bold']))\n",
    "    if mapping_type == 'random':\n",
    "        print(mapping_type[0].upper() + mapping_type[1:] + \" \" + \"Primary Source Duplicate Fields:\", random_primary_duplicate)\n",
    "        print(mapping_type[0].upper() + mapping_type[1:] + \" \" + \"Secodnary Source Duplicate Fields:\", random_secondary_duplicate)\n",
    "    else:\n",
    "        print(mapping_type[0].upper() + mapping_type[1:] + \" \" + \"Primary Source Duplicate Fields:\", priority_primary_duplicate)\n",
    "        print(mapping_type[0].upper() + mapping_type[1:] + \" \" + \"Secodnary Source Duplicate Fields:\", priority_secondary_duplicate)\n",
    "        \n",
    "    if mapping_type == 'random':\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    print(colored(\"\\n\" + \"*\"*126 + \"\\n\", color, attrs=['bold']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee948b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_of_truth_explode(df_2):\n",
    "    columns_to_be_added = []\n",
    "    for col in df_2.columns:\n",
    "        if ((\"source_of_truth\" in col) & (len(col) > 14)):\n",
    "            df_2[col].apply(lambda x: columns_to_be_added.extend(list(list(zip(*x))[0])))\n",
    "    return set(columns_to_be_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd2b02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "f = open (r'C:\\Users\\nevintoms\\Desktop\\Field_Mapping_automation\\Input_Config\\input_details.json', \"r\")\n",
    "  \n",
    "# Reading from file\n",
    "data = json.loads(f.read())\n",
    "\n",
    "secondary_path = data['secondary_path']\n",
    "primary_path = data['primary_path']\n",
    "# secondary_path = secondary_path.replace(\"\\\\\", \"\\\\\")\n",
    "# primary_path = primary_path.replace(\"\\\\\", \"\\\\\")\n",
    "\n",
    "secondary_path_file_type = secondary_path.split(\".\")[-1]\n",
    "primary_path_file_type = primary_path.split(\".\")[-1]\n",
    "if secondary_path_file_type == \"csv\":\n",
    "    df_1 = pd.read_csv(secondary_path)\n",
    "elif secondary_path_file_type == \"json\":\n",
    "    df_1 = pd.read_json(secondary_path)\n",
    "else:\n",
    "    df_1 = pd.read_parquet(secondary_path)\n",
    "    \n",
    "if primary_path_file_type == \"csv\":\n",
    "    df_2 = pd.read_csv(primary_path)\n",
    "elif primary_path_file_type == \"json\":\n",
    "    df_2 = pd.read_json(primary_path)\n",
    "else:\n",
    "    df_2 = pd.read_parquet(primary_path)\n",
    "    \n",
    "df_1 = df_1.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_2 = df_2.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_1 = df_1.fillna(value=np.nan)\n",
    "df_2 = df_2.fillna(value=np.nan)\n",
    "\n",
    "matrix = data['matrix']\n",
    "random = data['random']\n",
    "priority = data['priority_from_secondary']\n",
    "priority_random = data['priority_from_secondary_random']\n",
    "col_to_be_deleted_df_1 = data['col_to_be_ignored_from_secondary']\n",
    "col_to_be_deleted_df_2 = data['col_to_be_ignored_from_primary']\n",
    "col_to_keep_df_1 = data['col_to_considered_from_secondary']\n",
    "col_to_keep_df_2 = data['col_to_considered_from_primary']\n",
    "secondary_duplicate_update = data['secondary_duplicate_update']\n",
    "primary_duplicate_update = data['primary_duplicate_update']\n",
    "inventory_remove_source_name = data[\"inventory_remove_source_name\"]\n",
    "inventory_source_of_truth_explode = data[\"inventory_source_of_truth_explode\"]\n",
    "\n",
    "if inventory_source_of_truth_explode==\"True\":\n",
    "    print(\"starting\")\n",
    "    columns_to_be_added = source_of_truth_explode(df_2)\n",
    "\n",
    "# df_1_name = secondary_path.split(\"\\\\\")[-1]\n",
    "# df_2_name = primary_path.split(\"\\\\\")[-1]\n",
    "\n",
    "# df_1.loc[:, 'unique_record_identifier'] = [str(i) for i in range(len(df_1))]\n",
    "# df_2.loc[:, 'unique_record_identifier'] = [str(i) for i in range(len(df_2))]\n",
    "\n",
    "# df_1_original = df_1.copy()\n",
    "# df_2_original = df_2.copy()\n",
    "\n",
    "# if len(inventory_remove_source_name) > 2:\n",
    "#     df_2 = df[df_2['source_of_truth'] != inventory_remove_source_name]\n",
    "\n",
    "# def arr_length_greater_than_zero(arr):\n",
    "#     if len(arr[1:-1].split(\",\")) > 0:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "# if (arr_length_greater_than_zero(col_to_keep_df_1)):\n",
    "#     col_to_keep_df_1 = col_to_keep_df_1[:-1]+',unique_record_identifier'+']'\n",
    "#     df_1 = columns_to_keep(df_1, col_to_keep_df_1)\n",
    "    \n",
    "# if (arr_length_greater_than_zero(col_to_keep_df_2)):\n",
    "#     col_to_keep_df_2 = col_to_keep_df_2[:-1]+',unique_record_identifier'+']'\n",
    "#     df_2 = columns_to_keep(df_2, col_to_keep_df_2)\n",
    "    \n",
    "# if (arr_length_greater_than_zero(col_to_be_deleted_df_1)):\n",
    "#     col_to_be_deleted = col_to_be_deleted_df_1[1:-1].split(\",\")\n",
    "#     col_to_be_deleted = [x.strip() for x in col_to_be_deleted]\n",
    "#     df_1 = field_remove(df_1, col_to_be_deleted)\n",
    "\n",
    "# if (arr_length_greater_than_zero(col_to_be_deleted_df_2)):\n",
    "#     col_to_be_deleted = col_to_be_deleted_df_2[1:-1].split(\",\")\n",
    "#     col_to_be_deleted = [x.strip() for x in col_to_be_deleted]\n",
    "#     df_2 = field_remove(df_2, col_to_be_deleted)\n",
    "    \n",
    "# if len(ast.literal_eval(secondary_duplicate_update)) > 0:\n",
    "#     secondary_duplicate_update = ast.literal_eval(secondary_duplicate_update)\n",
    "#     for duplicate_update in list(secondary_duplicate_update.keys()):\n",
    "#         df_1[secondary_duplicate_update[duplicate_update]].fillna('1840-09-23T12:28:49.664010Z')\n",
    "#         df_1 = df_1.sort_values(secondary_duplicate_update[duplicate_update]).groupby(duplicate_update).tail(1)\n",
    "          \n",
    "# if len(ast.literal_eval(primary_duplicate_update)) > 0:\n",
    "#     primary_duplicate_update = ast.literal_eval(primary_duplicate_update)\n",
    "#     for duplicate_update in list(primary_duplicate_update.keys()):\n",
    "#         df_2[primary_duplicate_update[duplicate_update]].fillna('1840-09-23T12:28:49.664010Z')\n",
    "#         df_2 = df_2.sort_values(primary_duplicate_update[duplicate_update]).groupby(duplicate_update).tail(1)\n",
    "\n",
    "# # df_2 = df_2.sort_values('last_logon').groupby('employee_id').tail(1)\n",
    "\n",
    "# print(\"Message: Cleaning Started\")\n",
    "# df_1, i = clean_up(df_1)\n",
    "# df_2, i = clean_up(df_2, i)\n",
    "# print(\"Message: Cleaning Completed\\n\")\n",
    "\n",
    "# if matrix == 'True':\n",
    "#     mapping_matrix(df_1,df_2)\n",
    "#     print(\"Message: Mapping Matrix Completed\\n\")\n",
    "    \n",
    "# if random == 'True':\n",
    "#     print(colored(\"\\n\" + \"*\"*56 + \"RANDOM MAPPING\" + \"*\"*56, 'green', attrs=['bold']))\n",
    "#     final_arr=[]\n",
    "#     coverage, df_1_not_mapped, df_2_not_mapped, final_arr, df_1_mapped_unique_indentifier, df_2_mapped_unique_indentifier = random_mapping(df_1, df_2, 0, final_arr)\n",
    "#     random_secondary_not_mapped = df_1_original[df_1_original['unique_record_identifier'].isin(df_1_not_mapped['unique_record_identifier'])]\n",
    "#     random_primary_not_mapped = df_2_original[df_2_original['unique_record_identifier'].isin(df_2_not_mapped['unique_record_identifier'])]\n",
    "#     t1 = time.time()\n",
    "#     time_taken = round((t1-t0)/60,2)\n",
    "#     mapping(final_arr, df_1_name, df_2_name)\n",
    "#     total_duplicate_records = duplicate_field_details('random', final_arr)\n",
    "#     mapping_summary(df_1_name, df_1, coverage, len(total_duplicate_records))\n",
    "#     duplicate_records_summary('random')\n",
    "    \n",
    "# priority_temp = priority[1:-1].split()\n",
    "# if ((len(priority_temp ) > 0)):\n",
    "#     print(colored(\"\\n\" + \"*\"*55 + \"PRIORITY MAPPING\" + \"*\"*55, 'red', attrs=['bold']))\n",
    "#     priority = priority[1:-1].split(\",\")\n",
    "#     priority = list(map(lambda x: x.lower(), priority))\n",
    "#     df_1_not_mapped = df_1.copy()\n",
    "#     df_2_not_mapped = df_2.copy()\n",
    "#     coverage=0\n",
    "#     final_arr=[]\n",
    "#     df_1_mapped_unique_indentifier = []\n",
    "#     df_2_mapped_unique_indentifier = []\n",
    "#     for priority_item in priority:\n",
    "#         priority_item = priority_item.strip()\n",
    "#         priority_item = priority_item+\",\"+'unique_record_identifier'\n",
    "#         priority_item_modified = \"[\" + priority_item + \"]\"\n",
    "#         df_1_priority = columns_to_keep(df_1_not_mapped, priority_item_modified)\n",
    "#         coverage, df_1_not_mapped, df_2_not_mapped, final_arr, df_1_mapped_unique_indentifier_priority, df_2_mapped_unique_indentifier_priority = random_mapping(df_1_priority, df_2_not_mapped, coverage, final_arr)\n",
    "#         df_1_not_mapped = df_1[df_1['unique_record_identifier'].isin(df_1_not_mapped['unique_record_identifier'])]\n",
    "      \n",
    "#         if len(df_1_mapped_unique_indentifier_priority) > 0:\n",
    "#             df_1_mapped_unique_indentifier.extend(df_1_mapped_unique_indentifier_priority)\n",
    "#         if len(df_2_mapped_unique_indentifier_priority) > 0:\n",
    "#             df_2_mapped_unique_indentifier.extend(df_2_mapped_unique_indentifier_priority)\n",
    "#     if priority_random == 'True':\n",
    "#         coverage, df_1_not_mapped, df_2_not_mapped, final_arr, df_1_mapped_unique_indentifier_priority, df_2_mapped_unique_indentifier_priority = random_mapping(df_1_not_mapped, df_2_not_mapped, coverage, final_arr)\n",
    "#         df_1_mapped_unique_indentifier.extend(df_1_mapped_unique_indentifier_priority)\n",
    "#         df_2_mapped_unique_indentifier.extend(df_2_mapped_unique_indentifier_priority)\n",
    "    \n",
    "#     priority_secondary_not_mapped = df_1_original[df_1_original['unique_record_identifier'].isin(df_1_not_mapped['unique_record_identifier'])]\n",
    "#     priority_primary_not_mapped = df_2_original[df_2_original['unique_record_identifier'].isin(df_2_not_mapped['unique_record_identifier'])]\n",
    "#     mapping(final_arr, df_1_name, df_2_name)\n",
    "#     total_duplicate_records = duplicate_field_details('priority', final_arr)\n",
    "#     mapping_summary(df_1_name, df_1, coverage, len(total_duplicate_records))\n",
    "#     duplicate_records_summary('priority')\n",
    "    \n",
    "# print(colored(\"\\nVariable Name Details:\", attrs=['bold']))\n",
    "# print(\"Mapped Details of Primary/Secondary Source: (random/priority)_(primary/secondary)_mapped\")\n",
    "# print(\"Unmapped Deatails of Primary/Secodnary Source: (random/priority)_(primary/secondary)_not_mapped\")\n",
    "# print(\"Individual Mapping Details: (random/priority)_(primary/secondary)_mapped_(mapping_field_name)\")\n",
    "# print(\"Duplicate mapping details: (random/priority)_(primary/secondary)_mapped_(mapping_field_name)_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0aa41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_to_be_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0528b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[('SF', 'nevin'), ('AD', 'TOMS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(*[('a', 1), ('b', 2), ('c', 3), ('d', 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_parquet(r\"C:\\Users\\Public\\Downloads\\SummaryDM\\12Oct2022\\INV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e606ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_added = []\n",
    "for col in df_2.columns:\n",
    "    if ((\"source_of_truth\" in col) & (len(col) > 14)):\n",
    "        print(col)\n",
    "        for x in df_2[col]:\n",
    "            if x is not None:\n",
    "                print(x)\n",
    "                columns_to_be_added.extend(list(list(zip(*x))[0]))\n",
    "#         df_2[col].apply(lambda x: columns_to_be_added.extend(list(list(zip(*x))[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad377657",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(columns_to_be_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['object_name_with_source_of_truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba8664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in zip(*a):\n",
    "    print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80ab7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tu = set(list(zip(*a))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tu.add(\"AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4996d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac776a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_primary_mapped_employee_id_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136aaca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_secondary_mapped[priority_secondary_mapped['employee_id']=='14200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_primary_mapped[priority_primary_mapped['employee_id']=='14200'][['object_name', 'last_logon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9a830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed56746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
